\chapter{Deep Learning in Neural Encoding Prediction}

This section comprises an essential background in machine learning and deep learning techniques used in neuroscience, in particular in neural response prediction. We introduce regression, deep neural networks (DNNs), convolutional neural networks (CNNs) and rotation-equivariant CNNs. To broaden the knowledge of machine learning or deep learning, we refer readers to two excellent books: Pattern Recognition and Machine Learning \citep{bishop2006pattern} and Deep Learning \citep{Goodfellow-et-al-2016}.

\section{Regression}

Regression is a supervised machine learning problem that is also often encountered in neuroscience, for example, in predicting a neural response given data input. Firstly, let us define what regression is.

\begin{defn}[Training dataset]\label{def01:1}
	Let $P_{data}(Y|X)$ be a data generating distribution where $Y$ is a random variable and $X = (X^{(1)}, … , X^{(d)})$ a multivariate random variable. Let $D = {(X_1, Y_1), \dots, (X_n, Y_n)}$ be an independently drawn sample of size $n$ from $P_{data}$, where we assume that $(X_i, Y_i)$ are independent and identically distributed random variables. We then refer to $D$ as the training dataset. Let us also denote the empirical data distribution described by $D$ by $\hat{p}_{data}$.
\end{defn}

\begin{defn}[Loss function]\label{def01:2}
Given a function $f: \mathbb{R}^d \mapsto \mathbb{R}$, the loss function is a function $L: (D \times f) \mapsto R$ which is used to assess the quality of the function $f$ given the training dataset $D$.
\end{defn}

\begin{defn}[Regression]\label{def01:3}
Given a training dataset $D$, regression is a task of finding a function $f: \mathbb{R}^d \mapsto \mathbb{R}$ that predicts $Y$ given $X$ from $P_{data}(Y|X)$. This problem is solved by minimizing the loss function $L: (D \times f) \mapsto \mathbb{R}$, whose purpose is to assess the quality of prediction of $Y$ from the data generating distribution by function $f$ given the training dataset $D$. The regression task is, therefore, to find $f$ such that $f = argmin_f L(D, f)$. 
\end{defn}

Notice that even though a sample of only size $n$ is available, the task of regression is to predict $Y$ given $X$ sampled from $P_{data}(Y|X)$, which is the original data distribution. To obtain $f$ modeling $P(Y|X)$ as accurately as possible, the size of the training dataset is, therefore, critical.

\begin{defn}[Hypothesis and Hypotheses space]\label{def01:4}
The function $f \in F$ is commonly called a hypothesis and $F$ a hypothesis space from which we choose the hypothesis (for example, all linear functions, all polynomials, etc.). In our case, the hypothesis space is described by variables called weights (or parameters), which we will denote by $w$. Finding $f \in F$ is then equivalent to finding the proper weights. We will, therefore, often write $f(w)$ instead of $f$ to emphasize that $f$ is parametrized by $w$. Moreover, we will write $L(D, w)$ instead of $L(D, f)$ since $f$ is fully determined by $w$.
\end{defn}

It is often beneficial to think of $Y$ in a sample $(X, Y) \sim P_{data}$ as a sum of two random variables: $Y = Y_{explainable} + Y_{noise}$. The former is generated by a distribution that can be fully explained by some mathematical function $g(X, w)$, that is $g(X, w) = \mathbb{E}_{Y|X}[Y]$, where $(X, Y) \sim P_{data}$. On the other hand, the random variable $Y_{noise}$ is independent of $X$ and generated by a noise-generating distribution. 

Assuming regression with a commonly used MSE loss function (defined below), this consequently leads to decomposition of the model’s loss into two summands \citep{bishop2006pattern}:
\begin{equation}
\mathbb{E}_{X, Y}[L] = 
\mathbb{E}_{X, Y}[(f(X, w) - g(X))^2)] + 
\mathbb{E}_{X, Y}[(g(X) - Y)^2]
\end{equation}

Our model minimizes the first term, while the second term is defined by noise which the model cannot change. For this reason, literature refers to this as an irreducible error, giving the lower bound of the overall loss.

The noise can be caused by the measuring device, or more importantly, in our case, it can depend on some hidden features not provided in our dataset. The machine learning model can never predict the noise distribution. This is not caused by a poor choice of a model but by the lack of information on which the variable $Y$ is dependent.

In regression, it is often assumed that $Y_{noise}$ is generated by Gaussian distribution. Moreover, the normal distribution is a distribution with the highest entropy (between all distributions having fixed mean and variance), meaning it is the most general distribution with these properties. If we used maximum likelihood estimation to estimate the parameters $w$, we would arrive at a frequently used loss function in machine learning; mean squared error.


\begin{defn}[Mean Squared Error (MSE)]\label{def01:5}
	Mean squared error loss function is defined as:
	\begin{equation}
	L(D, w) = \frac{1}{n} \sum_{i=1}^n (f(x_i) - y_i)^2,
	\end{equation}
	where $(X_i, Y_i) \in D$ for $1 \leq i \leq n$.
\end{defn}

However, in the prediction of neural responses, the normal distribution is not appropriate mainly due to the following two reasons:
\begin{enumerate}
	\item Neuron’s response is assumed to be non-negative as it correlates with its firing rate. Thus we do not need to use the most general distribution. Gaussian distribution is unsuitable for this task since it can generate negative numbers.
	\item Variance of neuron’s response correlates with its firing rate \citep{goris2014partitioning}, meaning that the random variable $Y_{noise}$ is dependent on the absolute value of $Y_{explainable}$. Using a distribution whose variance increases linearly with the mean would be beneficial.
\end{enumerate}


These two reasons make Poisson distribution more appropriate in neural response prediction. Therefore, the Poisson loss, obtained from maximum likelihood estimation, is often used \citep{cadena2019deep}, \citep{klindt2017neural}, \citep{sinz2018stimulus}, \citep{lurz2021generalization}:

\begin{defn}[Poisson loss]\label{def01:6}
	\begin{equation}
		L_{Poiss}((X, y), w) = \frac{1}{m} \sum_{i=1}^m (f(x_i) - y_i log(f(x_i)),
	\end{equation}
	where $m$ stands for the number of neurons.
\end{defn}


\section{Regularization and Overfitting}

In machine learning, the objective is to capture all necessary patterns in the training dataset and perform well on previously unseen inputs. If the loss function contains only terms that minimize the error on training data, we might obtain a model performing perfectly on the training dataset, learning even patterns caused by noise, consequently leading to poor performance on other unseen data. This phenomenon, called \emph{overfitting}, is a typical result of machine learning models with little or no regularization. Overfitting often occurs in the presence of a small training dataset, which is typically the case in neuroscience.


Regularization is a technique minimizing the \emph{generalization error}, the error on previously unseen data. For this reason, among many other techniques for model regularization, new terms are usually added to the loss function to penalize functions that do not seem general enough and tend to overfit.

Usual regularization terms are the $L_1$ and $L_2$ norms of weights $w$, which favor simpler functions and thus prevent overfitting on the training dataset. The norms are defined as follows:

\begin{equation}
L_1(w) = \sum_i |w_i|
\end{equation}

\begin{equation}
L_2(w) = \sum_i w_i^2
\end{equation}

Finally, to regularize our model, we add the regularization terms to the loss function:

\begin{equation}
L_{reg}(D, w) = L(D, w) + \lambda_1 L_1(w) + \lambda_2 L_2(w),
\end{equation}
where $\lambda_1$ and $\lambda_2$ are parameters not tuned by the model but instead set previously by the author’s choice. Parameters that have to be previously set and are not fitted by the algorithm are called \emph{hyperparameters}. Hyperparameters are often chosen based on the performance on the validation dataset; a separate previously unseen dataset used to perform validation of our model and to tune the hyperparameters. It is crucial that the model did not previously see this data because one of the most important information we get from the validation dataset is whether our model is overfitted or not.


\section{Deep Neural Networks}

In recent years, deep neural networks (DNNs)\footnote{Do not confuse with biological neural networks in the brain. Deep neural networks (also called deep artificial neural networks) were historically inspired by the human brain. One node in DNN, sometimes called a perceptron, unit, or neuron, should correspond to a simplified model of a cortical neuron \citep{rosenblatt1958perceptron}. In this work, we try to avoid the term neuron due to the confusion it might cause.} gained respect in the computer science community due to their high performance on various tasks, outperforming many state-of-the-art solutions. Two crucial factors primarily caused DNNs to dominate previous solutions. Firstly, the higher availability of computational power enabled the world to train very deep neural networks on large datasets. Secondly, in 1989 the Universal Approximation Theorem was proven \citep{cybenko1989approximation}. In this paper, the author proved that under certain conditions, any function $g: [0, 1]^d \mapsto R$ can be arbitrarily closely approximated by a multilayer perceptron, an elementary type of artificial neural network. In this particular proof, the most limiting condition was a specific type of activation function along with the network architecture (more on this in the following sections). However, later papers broadened the classes of activation functions and DNN architectures \citep{leshno1993multilayer}, \citep{heinecke2020refinement} \citep{zhou2020universality}, giving a solid theoretical underpinning to the whole field of deep neural networks; also called deep learning.

Deep neural networks nowadays also dominate the field of neural response prediction. Our approach builds upon the current DNN architectures. Before getting into details, let us describe an artificial neural network.

\section{Feed-Forward Deep Neural Networks}

A DNN is a machine learning model built of interconnected artificial neurons called perceptrons, units, or nodes. We limit the large class of neural networks to feed-forward DNNs, networks that do not contain a cycle. In terms of already defined notions, we can think of feed-forward DNNs as a hypothesis space defined by the network’s hyperparameters and weights. An artificial neuron can be described as a mathematical function that performs a weighted sum of its inputs that is passed to an activation function. This activation function is usually non-linear so the whole neuron is not just a linear mapping. Mathematically, the output of one node is computed as follows: 

\begin{equation}
u_{out} = a \left( \sum_{i=1}^k w_i u_{in_i} \right),
\end{equation}
where $u$ stands for the particular neuron and $w_i$ are the weights of the connections into the neuron $u$. This neuron $u$ has $k$ inputs.

Artificial neural networks are usually organized into layers of neurons. Since current models use many layers stacked one after another, the networks are called deep neural networks. Every layer is a function that transforms an input tensor $x$ to an output tensor $y$. We refer to the connections between layers, their sizes, and other properties of the layers and the whole network as the architecture of the DNN model. Concrete numbers specifying the model’s architecture belong to the model’s hyperparameters. 

A typically used deep learning layer, often called the fully connected layer, has every neuron connected to every input (usually being the neurons of the previous layer). Mathematically, the unit’s $j$-th output is computed as follows: 

\begin{equation}
y_j = a\left(\sum_{i=1}^k w_{ji} u_{in_i}\right),
\end{equation}
where $w_ji$ is the weight associated with the $i$-th neuron from the previous layer and a is the activation function.

We obtain a basic deep neural network by stacking $q$ fully connected layers after each other. Its output can be computed as $y = l_1(\dots l_q(x) \dots)$, where $l_i$ is a transformation performed by the $i$-th layer. In these simple sequential architectures, the first layer is called the input layer, the last layer is usually refered to as the output layer, and all the layers in between called the hidden layers (fig DNN TODO)






