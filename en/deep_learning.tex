\chapter{Deep Learning in Neural Encoding Prediction}

This section comprises an essential background in machine learning and deep learning techniques used in neuroscience, in particular in neural response prediction. We introduce regression, deep neural networks (DNNs), convolutional neural networks (CNNs) and rotation-equivariant CNNs. To broaden the knowledge of machine learning or deep learning, we refer readers to two excellent books: Pattern Recognition and Machine Learning \citep{bishop2006pattern} and Deep Learning \citep{Goodfellow-et-al-2016}.

\section{Regression}

Regression is a supervised machine learning problem that is also often encountered in neuroscience, for example, in predicting a neural response given data input. Firstly, let us define what regression is.

\begin{defn}[Training dataset]\label{def01:1}
	Let $P_{data}(Y|\textbf{X})$ be a data generating distribution where $Y$ is a random variable and $\textbf{X} = (X_1, … , X_d)$ a multivariate random variable. Let $D = {(\textbf{X}_1, Y_1), \dots, (\textbf{X}_n, Y_n)}$ be an independently drawn sample of size $n$ from $P_{data}$, where we assume that $(\textbf{X}_i, Y_i)$ are independent and identically distributed random variables. We then refer to $D$ as the training dataset. Let us also denote the empirical data distribution described by $D$ by $\hat{p}_{data}$.
\end{defn}

\begin{defn}[Loss function]\label{def01:2}
Given a function $f: \mathbb{R}^d \mapsto \mathbb{R}$, the loss function is a function $L: (D \times f) \mapsto R$ which is used to assess the quality of the function $f$ given the training dataset $D$.
\end{defn}

\begin{defn}[Regression]\label{def01:3}
Given a training dataset $D$, regression is a task of finding a function $f: \mathbb{R}^d \mapsto \mathbb{R}$ that predicts $Y$ given $\textbf{X}$ from $P_{data}(Y|\textbf{X})$. This problem is solved by minimizing the loss function $L: (D \times f) \mapsto \mathbb{R}$, whose purpose is to assess the quality of prediction of $Y$ from the data generating distribution by function $f$ given the training dataset $D$. The regression task is, therefore, to find $f$ such that $f = argmin_f L(D, f)$. 
\end{defn}

Notice that even though a sample of only size $n$ is available, the task of regression is to predict $Y$ given $\textbf{X}$ sampled from $P_{data}(Y|\textbf{X})$, which is the original data distribution. To obtain $f$ modeling $P(Y|\textbf{X})$ as accurately as possible, the size of the training dataset is, therefore, critical.

\begin{defn}[Hypothesis and Hypotheses space]\label{def01:4}
The function $f \in F$ is commonly called a hypothesis and $F$ a hypothesis space from which we choose the hypothesis (for example, all linear functions, all polynomials, etc.). In our case, the hypothesis space is described by variables called weights (or parameters), which we will denote by $w$. Finding $f \in F$ is then equivalent to finding the proper weights. We will, therefore, often write $f(w)$ instead of $f$ to emphasize that $f$ is parametrized by $w$. Moreover, we will write $L(D, w)$ instead of $L(D, f)$ since $f$ is fully determined by $w$.
\end{defn}

It is often beneficial to think of $Y$ in a sample $(\textbf{X}, Y) \sim P_{data}$ as a sum of two random variables: $Y = Y_{explainable} + Y_{noise}$. The former is generated by a distribution that can be fully explained by some mathematical function $g(\textbf{X}, w)$, that is $g(\textbf{X}, w) = \mathbb{E}_{Y|\textbf{X}}[Y]$, where $(\textbf{X}, Y) \sim P_{data}$. On the other hand, the random variable $Y_{noise}$ is independent of $\textbf{X}$ and generated by a noise-generating distribution. 

Assuming regression with a commonly used MSE loss function (defined below), this consequently leads to decomposition of the model’s loss into two summands \citep{bishop2006pattern}:
\begin{equation}
\mathbb{E}_{\textbf{X}, Y}[L] = 
\mathbb{E}_{\textbf{X}, Y}[(f(\textbf{X}, w) - g(\textbf{X}))^2)] + 
\mathbb{E}_{\textbf{X}, Y}[(g(\textbf{X}) - Y)^2]
\end{equation}

Our model minimizes the first term, while the second term is defined by noise which the model cannot change. For this reason, literature refers to this as an irreducible error, giving the lower bound of the overall loss.

The noise can be caused by the measuring device, or more importantly, in our case, it can depend on some hidden features not provided in our dataset. The machine learning model can never predict the noise distribution. This is not caused by a poor choice of a model but by the lack of information on which the variable $Y$ is dependent.

In regression, it is often assumed that $Y_{noise}$ is generated by Gaussian distribution. Moreover, the normal distribution is a distribution with the highest entropy (between all distributions having fixed mean and variance), meaning it is the most general distribution with these properties. If we used maximum likelihood estimation to estimate the parameters $w$, we would arrive at a frequently used loss function in machine learning; mean squared error.

Definition (): 

\begin{defn}[Mean Squared Error (MSE)]\label{def01:5}
	Mean squared error loss function is defined as:
	\begin{equation}
		L((\textbf{X}, Y), w) = \frac{1}{n} \sum_{i=1}^n (f(x_i) - y_i)^2
	\end{equation}
\end{defn}

